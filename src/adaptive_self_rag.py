"""
Adaptive_self_rag
ê¸ˆìœµìƒí’ˆ(ì˜ˆ: ì •ê¸°ì˜ˆê¸ˆ, ì…ì¶œê¸ˆììœ ì˜ˆê¸ˆ) ê´€ë ¨ ì§ˆì˜ì— ëŒ€í•´:
1. ì§ˆë¬¸ ë¼ìš°íŒ… â†’ (ê¸ˆìœµìƒí’ˆ ê´€ë ¨ì´ë©´) ë¬¸ì„œ ê²€ìƒ‰ (ë³‘ë ¬ ì„œë¸Œ ê·¸ë˜í”„) â†’ ë¬¸ì„œ í‰ê°€ â†’ (ì¡°ê±´ë¶€) ì§ˆë¬¸ ì¬ì‘ì„± â†’ ë‹µë³€ ìƒì„±
   / (ê¸ˆìœµìƒí’ˆê³¼ ë¬´ê´€í•˜ë©´) LLM fallbackì„ í†µí•´ ë°”ë¡œ ë‹µë³€ ìƒì„±
ê·¸ë¦¬ê³  ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆ(í™˜ê°, ê´€ë ¨ì„±) í‰ê°€ í›„ í•„ìš”ì‹œ ì¬ìƒì„± ë˜ëŠ” ì¬ì‘ì„±í•˜ëŠ” Adaptive Self-RAG ì²´ì¸.
"""


#############################
# 1. ê¸°ë³¸ í™˜ê²½ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
#############################

from dotenv import load_dotenv
load_dotenv()

import warnings
warnings.filterwarnings("ignore")

# ê¸°íƒ€ ìœ í‹¸
import json
import uuid
import re
from textwrap import dedent
from operator import add
from heapq import merge
from typing import List, Literal, Sequence, TypedDict, Annotated, Tuple

# ì„œì¹˜ ì•Œê³ ë¦¬ì¦˜
from rank_bm25 import BM25Okapi

# LangChain, Chroma, LLM ê´€ë ¨
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.tools import tool 
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage
from langgraph.checkpoint.memory import MemorySaver
from langchain_community.tools import TavilySearchResults
from langchain_core.runnables import RunnableLambda

# Grader í‰ê°€ì§€í‘œìš©
from pydantic import BaseModel, Field

# ê·¸ë˜í”„ ê´€ë ¨
from langgraph.graph import StateGraph, START, END

# Gradio ê´€ë ¨
import gradio as gr

#############################
# 2. ì„ë² ë”©, DB ì„¤ì •, ì„œì¹˜ ì¸ë±ìŠ¤ ìƒì„±
#############################
from langchain_core.embeddings import Embeddings
from sentence_transformers import SentenceTransformer
import torch 

class LangChainSentenceTransformer(Embeddings):
    def __init__(self, model_name: str):
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"[INFO] Using device: {device}")
        self.model = SentenceTransformer(model_name, device=device)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self.model.encode(texts, show_progress_bar=False, convert_to_numpy=True).tolist()

    def embed_query(self, text: str) -> List[float]:
        return self.model.encode(text, show_progress_bar=False, convert_to_numpy=True).tolist()

embeddings_model_koMultitask = LangChainSentenceTransformer("jhgan/ko-sroberta-multitask") # 768ì°¨ì› ì„ë°°ë”© ëª¨ë¸ë¡œ ë³€ê²½

# Chroma DB ê²½ë¡œ
CHROMA_DIR = "./../findata/chroma_db"

# JSON ë°ì´í„° ê²½ë¡œ
FIXED_JSON_PATH = "./../findata/fixed_deposit.json"
DEMAND_JSON_PATH = "./../findata/demand_deposit.json"
LOAN_JSON_PATH = "./../findata/loan.json"
SAVINGS_JSON_PATH = "./../findata/savings.json"


# DB ì´ë¦„
FIXED_COLLECTION = "fixed_deposit"
DEMAND_COLLECTION = "demand_deposit"
LOAN_COLLECTION = "loan"
SAVINGS_COLLECTION = "savings"


def load_and_prepare_all_documents(json_paths: list[str]) -> list[Document]:
    docs: list[Document] = []
    for path in json_paths:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        for entry in data["documents"]:
            # ë³¸ë¬¸
            content = entry["content"]
            # metadata í•„ë“œ ì˜ì–´ í‚¤ë¡œ í†µì¼
            md = entry.get("metadata", {})
            bank         = entry.get("bank",         md.get("ì€í–‰"))
            product_name = entry.get("product_name", md.get("ìƒí’ˆëª…"))
            category     = entry.get("type")
            docs.append(
                Document(
                    page_content=content,
                    metadata={
                        "id":           entry.get("id"),
                        "type":         category,
                        "bank":         bank,
                        "product_name": product_name
                    }
                )
            )
    return docs

# JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
ALL_JSON = [
    FIXED_JSON_PATH,
    DEMAND_JSON_PATH,
    LOAN_JSON_PATH,
    SAVINGS_JSON_PATH,
]

all_documents = load_and_prepare_all_documents(ALL_JSON)
corpus = [doc.page_content.split() for doc in all_documents]
bm25_index = BM25Okapi(corpus)

# ì¸ì œìŠ¤ì²œ
vector_db = Chroma(
    embedding_function=embeddings_model_koMultitask,
    collection_name="combined_products_koMultitask",  # ì„ë°°ë”© ëª¨ë¸ì— ë”°ë¥¸ ì¸ì œìŠ¤ì²œ ì´ë¦„ ë³€ê²½
    persist_directory=CHROMA_DIR,
)

# í•œ ë²ˆë§Œ ì¸ì œìŠ¤ì²œ
if not vector_db._collection.count():
    print("DBì— ë¬¸ì„œ ì—†ìŒìœ¼ë¡œ ì¸ì œìŠ¤ì²œ ì§„í–‰")
    vector_db.add_documents(all_documents)

#ì¸ì œìŠ¤ì²œ í™•ì¸
print("ì´ ë¬¸ì„œ ìˆ˜:", len(all_documents))
from collections import Counter
print(Counter(doc.metadata['type'] for doc in all_documents))

def extract_bank(query: str) -> str | None:
    m = re.search(r'([ê°€-í£A-Za-z0-9]+?(ì€í–‰|ë±…í¬|íšŒì‚¬))', query)
    return m.group(1) if m else None

def extract_product(query: str) -> str | None:
    # â€œ~í†µì¥â€, â€œ~ì˜ˆê¸ˆâ€, â€œ~ëŒ€ì¶œâ€ ë“±ìœ¼ë¡œ ë§ˆì¹¨
    m = re.search(r'([ê°€-í£A-Za-z0-9]+(?:í†µì¥|ì˜ˆê¸ˆ|ëŒ€ì¶œ))', query)
    return m.group(1).strip() if m else None


#############################
# 3. ì„œì¹˜ì•Œê³ ë¦¬ì¦˜ ë° ë„êµ¬(ê²€ìƒ‰ í•¨ìˆ˜) ì •ì˜
#############################

def _search_with_filters(query: str, filters: dict, top_k: int) -> list[Document]:
    # 1) BM25: ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ score ê³„ì‚° â†’ metadata í•„í„° ì ìš©í•´ top_k
    tokenized = query.split()
    scores = bm25_index.get_scores(tokenized)
    idxs   = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)
    bm25_docs = []
    for i in idxs:
        d = all_documents[i]
        if all(filters.get(k) is None or d.metadata.get(k)==filters[k] for k in filters):
            bm25_docs.append(d)
            if len(bm25_docs)>=top_k: break

    # 2) ë²¡í„°: Chromaì˜ filter íŒŒë¼ë¯¸í„° ì‚¬ìš© (ë‹¨ì¼í‚¤ vs ë‹¤ì¤‘í‚¤ì— ë”°ë¼ $and ë¡œ ë¬¶ì–´ì„œ ì „ë‹¬)
    meta = {k: v for k, v in filters.items() if v is not None}
    if not meta:
        vec_docs = vector_db.similarity_search(query, k=top_k)
    elif len(meta) == 1:
        # ë‹¨ì¼ í•„í„°í„°
        vec_docs = vector_db.similarity_search(query, k=top_k, filter=meta)
    else:
        # ì—¬ëŸ¬ í•„í„°ëŠ” í•˜ë‚˜ì˜ ì—°ì‚°ì($and)ë¡œ ë¬¶ì–´ì„œ ë„˜ê²¨ì•¼ í•¨
        and_list = [{k: v} for k, v in meta.items()]
        vec_docs = vector_db.similarity_search(
                    query, 
                    k=top_k, 
                    filter={"$and": and_list}
        )

    # 3) ì¤‘ë³µ ì œê±° ë° PDF ë§í¬ ì¶”ê°€ ì²˜ë¦¬
    seen, merged = set(), []
    for d in bm25_docs + vec_docs:
        uid = d.metadata["id"]
        if uid not in seen:
            seen.add(uid)
            # PDF ë§í¬ê°€ ìˆìœ¼ë©´ page_contentì— ì¶”ê°€
            pdf = d.metadata.get("pdf_link")
            if pdf and "pdf_link" not in d.page_content:
                d.page_content += f"\n\nğŸ“„ [ìƒí’ˆì„¤ëª…ì„œ PDF ë³´ê¸°]({pdf})"
            merged.append(d)
    return merged


# í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ ì•Œê³ ë¦¬ì¦˜
def hybrid_core_search(query: str, category: str, bank: str=None, product_name: str=None, top_k: int=3) -> List[Document]:
    # 1) ë©”íƒ€ í•„í„° ì¤€ë¹„ (category í•„ìˆ˜ í¬í•¨)
    filters = {"type": category}
    if bank: 
        filters["bank"] = bank
    if product_name: 
        filters["product_name"] = product_name

    # 2) í•„í„° ë ˆë²¨ë³„ë¡œ ì ì§„ì  ê²€ìƒ‰
    filter_levels = [
        filters,
        {**filters, **{"product_name": None}},  # ìƒí’ˆëª… ì œì™¸
        {**filters, **{"bank": None}},          # ì€í–‰ ì œì™¸
        {"category": category}                  # ì¹´í…Œê³ ë¦¬ë§Œ
    ]

    # 3) ìˆœì„œëŒ€ë¡œ BM25+ë²¡í„° ë³‘ë ¬ ê²€ìƒ‰ â†’ ê²°ê³¼ ë°˜í™˜
    for flt in filter_levels:
        docs = _search_with_filters(query, flt, top_k=top_k)
        if docs:
            return docs
    return []

BANK_NORMALIZE = {
    "êµ­ë¯¼": "KBêµ­ë¯¼ì€í–‰",
    "êµ­ë¯¼ì€í–‰": "KBêµ­ë¯¼ì€í–‰",
    "ìˆ˜í˜‘": "Shìˆ˜í˜‘ì€í–‰",
    "ìˆ˜í˜‘ì€í–‰": "Shìˆ˜í˜‘ì€í–‰",
    "ì‹ í•œ": "ì‹ í•œì€í–‰",
    "ì‹ í•œì€í–‰": "ì‹ í•œì€í–‰",
    "ë†í˜‘": "NHë†í˜‘ì€í–‰",
    "NH": "NHë†í˜‘ì€í–‰",
    "ë†í˜‘ì€í–‰": "NHë†í˜‘ì€í–‰",
    "ìš°ë¦¬": "ìš°ë¦¬ì€í–‰",
    "ìš°ë¦¬ì€í–‰": "ìš°ë¦¬ì€í–‰",
    "IBK": "IBKê¸°ì—…ì€í–‰",
    "ê¸°ì—…ì€í–‰": "IBKê¸°ì—…ì€í–‰",
    "IBKê¸°ì—…ì€í–‰": "IBKê¸°ì—…ì€í–‰",
    "í•˜ë‚˜": "í•˜ë‚˜ì€í–‰",
    "í•˜ë‚˜ì€í–‰": "í•˜ë‚˜ì€í–‰",
    "ì¹´ì¹´ì˜¤": "ì¹´ì¹´ì˜¤ë±…í¬",
    "ì¹´ì¹´ì˜¤ë±…í¬": "ì¹´ì¹´ì˜¤ë±…í¬",
    "ë¶€ì‚°": "ë¶€ì‚°ì€í–‰",
    "ë¶€ì‚°ì€í–‰": "ë¶€ì‚°ì€í–‰",
    }

def get_banks_in_docs(documents: list[Document]) -> set[str]:
    banks = set()
    for doc in documents:
        bank = doc.metadata.get("bank", "")
        # IBKê¸°ì—…ì€í–‰, IBK, ê¸°ì—…ì€í–‰ ëª¨ë‘ ë§¤ì¹­í•  ìˆ˜ ìˆë„ë¡ ì²˜ë¦¬ í•„ìš”ì‹œ normalize
        if bank:
            banks.add(bank)
    return banks


def extract_and_normalize_banks(query: str) -> list[str]:
    found = set()
    for k in BANK_NORMALIZE:
        if k in query:
            found.add(BANK_NORMALIZE[k])
    return list(found)


@tool
def search_fixed_deposit(query: str) -> List[Document]:
    """
    Search for relevant fixed deposit (ì •ê¸°ì˜ˆê¸ˆ) product information using semantic similarity.
    This tool retrieves products matching the user's query, such as interest rates or terms.
    """
    bank, product = extract_bank(query), extract_product(query)
    return hybrid_core_search(query, category="ì •ê¸°ì˜ˆê¸ˆ", bank=bank, product_name=product)

@tool
def search_demand_deposit(query: str) -> List[Document]:
    """
    Search for demand deposit (ì…ì¶œê¸ˆììœ ì˜ˆê¸ˆ) product information using semantic similarity.
    This tool retrieves products matching the user's query, such as flexible withdrawal or interest features.
    """
    bank, product = extract_bank(query), extract_product(query)
    return hybrid_core_search(query, category="ì…ì¶œê¸ˆììœ ì˜ˆê¸ˆ", bank=bank, product_name=product)

@tool
def search_loan(query: str) -> List[Document]:
    """
    Search for loan (ëŒ€ì¶œ) product information using semantic similarity.
    This tool retrieves products matching the user's query, such as flexible withdrawal or interest features.
    """
    bank, product = extract_bank(query), extract_product(query)
    return hybrid_core_search(query, category="ëŒ€ì¶œ", bank=bank, product_name=product)

@tool
def search_savings(query:str) -> List[Document]:
    """
    Search for savings (ì ê¸ˆ) product information using semantic similarity.
    This tool retrieves products matching the user's query, such as flexible withdrawal or interest features.
    """
    bank, product = extract_bank(query), extract_product(query)
    return hybrid_core_search(query, category="ì ê¸ˆ", bank=bank, product_name=product)

@tool
def web_search(query: str) -> List[str]:
    """
    This tool serves as a supplementary utility for the financial product recommendation model.
    It retrieves up-to-date external information via web search using the Tavily API, 
    especially when relevant data is not available in the local vector databases

    Unlike the RAG-based tools that query embedded product databases,
    this tool is designed to handle broader or real-time questionsâ€”such as current interest rates, financial trends,
    or general queries outside the scope of structured deposit data.

    It returns the top 2 semantically relevant documents from the web.
    """

    tavily_search = TavilySearchResults(max_results=2)
    docs = tavily_search.invoke(query)

    formatted_docs = []
    for doc in docs:
        formatted_docs.append(
            Document(
                page_content= f'<Document href="{doc["url"]}"/>\n{doc["content"]}\n</Document>',
                metadata={"source": "web search", "url": doc["url"]}
                )
        )

    if len(formatted_docs) > 0:
        return formatted_docs
    
    return [Document(page_content="ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]


#############################
# 4. LLM ì´ˆê¸°í™” & ë„êµ¬ ë°”ì¸ë”©
#############################

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, streaming=True)

#############################
# 5. LLM ì²´ì¸ (Retrieval Grader / Hallucination / Answer Graders / Question Re-writer)
#############################

# (1) Retrieval Grader (ê²€ìƒ‰í‰ê°€)
class BinaryGradeDocuments(BaseModel):
    """Binary score for relevance check on retrieved documents."""
    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
    )

structured_llm_BinaryGradeDocuments = llm.with_structured_output(BinaryGradeDocuments)

system_prompt = """You are an expert in evaluating the relevance of search results to user queries.

[Evaluation criteria]
1. í‚¤ì›Œë“œ ê´€ë ¨ì„±: ë¬¸ì„œê°€ ì§ˆë¬¸ì˜ ì£¼ìš” ë‹¨ì–´ë‚˜ ìœ ì‚¬ì–´ë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
2. ì˜ë¯¸ì  ê´€ë ¨ì„±: ë¬¸ì„œì˜ ì „ë°˜ì ì¸ ì£¼ì œê°€ ì§ˆë¬¸ì˜ ì˜ë„ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í‰ê°€
3. ë¶€ë¶„ ê´€ë ¨ì„±: ì§ˆë¬¸ì˜ ì¼ë¶€ë¥¼ ë‹¤ë£¨ê±°ë‚˜ ë§¥ë½ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë¬¸ì„œë„ ê³ ë ¤
4. ë‹µë³€ ê°€ëŠ¥ì„±: ì§ì ‘ì ì¸ ë‹µì´ ì•„ë‹ˆë”ë¼ë„ ë‹µë³€ í˜•ì„±ì— ë„ì›€ë  ì •ë³´ í¬í•¨ ì—¬ë¶€ í‰ê°€

[Scoring]
- Rate 'yes' if relevant, 'no' if not
- Default to 'no' when uncertain

[Key points]
- Consider the full context of the query, not just word matching
- Rate as relevant if useful information is present, even if not a complete answer

Your evaluation is crucial for improving information retrieval systems. Provide balanced assessments.
"""
# ì±„ì  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ë¦¿
grade_prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "[Retrieved document]\n{document}\n\n[User question]\n{question}")
])

retrieval_grader_binary = grade_prompt | structured_llm_BinaryGradeDocuments

# (3) Hallucination Grader
class GradeHallucinations(BaseModel):
    """Binary score for hallucination present in generation answer."""
    binary_score: str = Field(
        description="Answer is grounded in the facts, 'yes' or 'no'"
    )

structured_llm_HradeHallucinations = llm.with_structured_output(GradeHallucinations)

# í™˜ê° í‰ê°€ë¥¼ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
halluci_system_prompt = """
You are an expert evaluator assessing whether an LLM-generated answer is grounded in and supported by a given set of facts.

[Your task]
    - Review the LLM-generated answer.
    - Determine if the answer is fully supported by the given facts.

[Evaluation criteria]
    - ë‹µë³€ì— ì£¼ì–´ì§„ ì‚¬ì‹¤ì´ë‚˜ ëª…í™•íˆ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ì •ë³´ ì™¸ì˜ ë‚´ìš©ì´ ì—†ì–´ì•¼ í•©ë‹ˆë‹¤.
    - ë‹µë³€ì˜ ëª¨ë“  í•µì‹¬ ë‚´ìš©ì´ ì£¼ì–´ì§„ ì‚¬ì‹¤ì—ì„œ ë¹„ë¡¯ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    - ì‚¬ì‹¤ì  ì •í™•ì„±ì— ì§‘ì¤‘í•˜ê³ , ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ì´ë‚˜ ì™„ì „ì„±ì€ í‰ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

[Scoring]
    - 'yes': The answer is factually grounded and fully supported.
    - 'no': The answer includes information or claims not based on the given facts.

Your evaluation is crucial in ensuring the reliability and factual accuracy of AI-generated responses. Be thorough and critical in your assessment.
"""
hallucination_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", halluci_system_prompt),
        ("human", "[Set of facts]\n{documents}\n\n[LLM generation]\n{generation}"),
    ]
)

hallucination_grader = hallucination_prompt | structured_llm_HradeHallucinations

# (4) Answer Grader 
class BinaryGradeAnswer(BaseModel):
    """Binary score to assess answer addresses question."""
    binary_score: str = Field(
        description="Answer addresses the question, 'yes' or 'no'"
    )

structured_llm_BinaryGradeAnswer = llm.with_structured_output(BinaryGradeAnswer)
grade_system_prompt = """
You are an expert evaluator tasked with assessing whether an LLM-generated answer effectively addresses and resolves a user's question.

[Your task]
    - Carefully analyze the user's question to understand its core intent and requirements.
    - Determine if the LLM-generated answer sufficiently resolves the question.

[Evaluation criteria]
    - ê´€ë ¨ì„±: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    - ì™„ì „ì„±: ì§ˆë¬¸ì˜ ëª¨ë“  ì¸¡ë©´ì´ ë‹¤ë¤„ì ¸ì•¼ í•©ë‹ˆë‹¤.
    - ì •í™•ì„±: ì œê³µëœ ì •ë³´ê°€ ì •í™•í•˜ê³  ìµœì‹ ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    - ëª…í™•ì„±: ë‹µë³€ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›Œì•¼ í•©ë‹ˆë‹¤.
    - êµ¬ì²´ì„±: ì§ˆë¬¸ì˜ ìš”êµ¬ ì‚¬í•­ì— ë§ëŠ” ìƒì„¸í•œ ë‹µë³€ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

[Scoring]
    - 'yes': The answer effectively resolves the question.
    - 'no': The answer fails to sufficiently resolve the question or lacks crucial elements.

Your evaluation plays a critical role in ensuring the quality and effectiveness of AI-generated responses. Strive for balanced and thoughtful assessments.
"""
answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", grade_system_prompt),
        ("human", "[User question]\n{question}\n\n[LLM generation]\n{generation}"),
    ]
)

answer_grader_binary = answer_prompt | structured_llm_BinaryGradeAnswer

# (5) Question Re-writer
def rewrite_question(question: str) -> str:
    """
    ì…ë ¥ ì§ˆë¬¸ì„ ë²¡í„° ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ì¬ì‘ì„±í•œë‹¤.
    """
    local_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    system_prompt = """
    You are an expert question re-writer. Your task is to convert input questions into optimized versions 
    for vectorstore retrieval. Analyze the input carefully and focus on capturing the underlying semantic 
    intent and meaning. Your goal is to create a question that will lead to more effective and relevant 
    document retrieval.

    [Guidelines]
        1. Identify and emphasize core concepts and key subjects.
        2. Expand abbreviations or ambiguous terms.
        3. Include synonyms or related terms that might appear in relevant documents.
        4. Maintain the original intent and scope.
        5. For complex questions, break them down into simpler, focused sub-questions.
    """
    re_write_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "[Initial question]\n{question}\n\n[Improved question]\n")
    ])
    question_rewriter = re_write_prompt | local_llm | StrOutputParser()
    rewritten_question = question_rewriter.invoke({"question": question})
    return rewritten_question

# (6) Generation Evaluation & Decision Nodes
def grade_generation_self(state: "SelfRagOverallState") -> str:
    print("--- ë‹µë³€ í‰ê°€ (ìƒì„±) ---")
    print(f"--- ìƒì„±ëœ ë‹µë³€: {state['generation']} ---")
    if state['num_generations'] > 1:
        print("--- ìƒì„± íšŸìˆ˜ ì´ˆê³¼, ì¢…ë£Œ -> end ---")
        return "end"
    # í‰ê°€ë¥¼ ìœ„í•œ ë¬¸ì„œ í…ìŠ¤íŠ¸ êµ¬ì„±
    print("--- ë‹µë³€ í• ë£¨ì‹œë„¤ì´ì…˜ í‰ê°€ ---")
    docs_text = "\n\n".join([d.page_content for d in state['documents']])
    hallucination_grade = hallucination_grader.invoke({
        "documents": docs_text,
        "generation": state['generation']
    })
    if hallucination_grade.binary_score == "yes":
        relevance_grade = retrieval_grader_binary.invoke({
            "question": state['question'],
            "document": state['filtered_documents'],
            "generation": state['generation']
        })
        print("--- ë‹µë³€-ì§ˆë¬¸ ê´€ë ¨ì„± í‰ê°€ ---")
        if relevance_grade.binary_score == "yes":
            print("--- ìƒì„±ëœ ë‹µë³€ì´ ì§ˆë¬¸ì„ ì˜ í•´ê²°í•¨ ---")
            return "useful"
        else:
            print("--- ë‹µë³€ ê´€ë ¨ì„±ì´ ë¶€ì¡± -> transform_query ---")
            return "not useful"
    else:
        print("--- ìƒì„±ëœ ë‹µë³€ì˜ ê·¼ê±°ê°€ ë¶€ì¡± -> generate ì¬ì‹œë„ ---")
        return "not supported"
    
def decide_to_generate_self(state: "SelfRagOverallState") -> str:
    print("--- í‰ê°€ëœ ë¬¸ì„œ ë¶„ì„ ---")
    if state['num_generations'] > 1:
        print("--- ìƒì„± íšŸìˆ˜ ì´ˆê³¼, ìƒì„± ê²°ì • ---")
        return "generate"
    # ì—¬ê¸°ì„œëŠ” í•„í„°ë§ëœ ë¬¸ì„œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not state['filtered_documents']:
        print("--- ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ -> transform_query ---")
        return "transform_query"
    else:
        print("--- ê´€ë ¨ ë¬¸ì„œ ì¡´ì¬ -> generate ---")
        return "generate"


# (7) RoutingDecision 
class RoutingDecision(BaseModel):
    """Determines whether a user question should be routed to document search or LLM fallback."""
    route: Literal["search_data","llm_fallback"] = Field(
        description="Classify the question as 'search_data' (financial) or 'llm_fallback' (general)"
        )

#############################
# 6. ìƒíƒœ ì •ì˜ ë° ë…¸ë“œ í•¨ìˆ˜ (ì „ì²´ Adaptive ì²´ì¸)
#############################

# ìƒíƒœ í†µí•©: SelfRagOverallState (ì§ˆë¬¸, ìƒì„±, ì›ë³¸ ë¬¸ì„œ, í•„í„° ë¬¸ì„œ, ìƒì„± íšŸìˆ˜)
# ë©”ì¸ ê·¸ë˜í”„ ìƒíƒœ ì •ì˜
class SelfRagOverallState(TypedDict):
    """
    Adaptive Self-RAG ì²´ì¸ì˜ ì „ì²´ ìƒíƒœë¥¼ ê´€ë¦¬    
    """
    question: str
    generation: Annotated[List[str], add]
    routing_decision: str
    num_generations: int
    documents: List[Document]
    filtered_documents: List[Document]
    history: List[Tuple[str,str]]     # (user, bot) ë©”ì‹œì§€ ìŒ ì €ì¥ìš©

def initialize_state() -> SelfRagOverallState:
    """Create a new state with proper initialization of all fields"""
    return {
        "question": "",
        "generation": [],
        "routing_decision": "",
        "num_generations": 0,
        "documents": [],
        "filtered_documents": [],
        "history": []
    }

# ìƒˆë¡œìš´ ì¬ì‘ì„± ì „ìš© LLM ì²´ì¸ - íˆìŠ¤í† ë¦¬ ë‹µë³€ì´ ìˆëŠ” ê²½ìš° ì´ì „ ëŒ€í™” ë§¥ë½ì— ë§ê²Œ ì§ˆë¬¸ì„ ìˆ˜ì •í•˜ì—¬ ë¬¸ì„œ ì„œì¹˜í•˜ê¸° ìœ„í•¨
def contextualize_query(state: SelfRagOverallState) -> dict:
    # ìµœê·¼ 3í„´ íˆìŠ¤í† ë¦¬ ì¶”ì¶œ
    recent = state['history'][-3:]
    hist_block = "\n".join(f"User: {u}\nAssistant: {a}" for u,a in recent)
    payload = {"history": hist_block, "question": state['question']}
    improved = question_rewriter_chain.invoke(payload)
    return {"question": improved}

rewrite_input = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ ê¸ˆìœµ ìƒí’ˆ ì±—ë´‡ AIì™€ ìœ ì €ì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ˆì§€ë§‰ ìœ ì €ì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê¸ˆìœµìƒí’ˆ ì¶”ì²œ RAG ì‹œìŠ¤í…œì´ ë¬¸ì„œë¥¼ ì˜ ì°¾ì„ ìˆ˜ ìˆê²Œ ì§ˆë¬¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”."
    "ì¬ì‘ì„±ëœ ì§ˆë¬¸ì€ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ í•˜ë©°, ìœ ì €ê°€ ì›íˆëŠ” í•µì‹¬ì´ ë¬´ì—‡ì¸ì§€ ëª…í™•í•˜ê²Œ ë“¤ì–´ë‚˜ëŠ” ë¬¸ìì´ì–´ì•¼ í•©ë‹ˆë‹¤."
    "ì ìš©ë˜ëŠ” RAGì˜ ì„œì¹˜ì•Œê³ ë¦¬ì¦˜ì€ ë°±í„°ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì„ ì¬ì‘ì„±í•˜ì„¸ìš”."
    "ë§Œì•½ [History]ì— ì•„ë¬´ê²ƒë„ ì—†ê±°ë‚˜ ìœ ì €ì˜ ë§ˆì§€ë§‰ ì§ˆì˜ê°€ ë§¥ë½ìƒ ê¸ˆìœµìƒí’ˆê³¼ ê´€ë ¨ëœ ê²ƒì´ ì•„ë‹ˆë¼ë©´ ìœ ì €ì˜ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì‘ì„±í•˜ì„¸ìš”."),
    ("system", "[History]\n{history}"),
    ("human", "[Question]\n{question}\n\n[Improved Question]\n"),
])
question_rewriter_chain = rewrite_input | llm | StrOutputParser()


# ì§ˆë¬¸ ì¬ì‘ì„± ë…¸ë“œ (ë³€ê²½ í›„ ê²€ìƒ‰ ë£¨í”„)
def transform_query_self(state: SelfRagOverallState) -> dict:
    print("--- ì§ˆë¬¸ ê°œì„  ---")
    new_question = rewrite_question(state['question'])
    print(f"--- ê°œì„ ëœ ì§ˆë¬¸ : \n{new_question} ")
    new_count = state['num_generations'] + 1
    print(f"num_generations : {new_count}")
    return {"question": new_question, "num_generations": new_count}

# ë‹µë³€ ìƒì„± ë…¸ë“œ (ì„œë¸Œ ê·¸ë˜í”„ë¡œë¶€í„° ë°›ì€ í•„í„° ë¬¸ì„œ ìš°ì„  ì‚¬ìš©, ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³  í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì •)
def format_chat_history(history):
    messages = []
    for user_msg, assistant_msg in history:
        messages.append(("human", user_msg))
        messages.append(("ai", assistant_msg))
    return messages

generate_template = ChatPromptTemplate.from_messages([
    ("system", 
     """
[Your task]
You are a financial product expert and consultant who always responds in Korean.
Analyze the user query and the given financial product data to recommend the most suitable product.
Use the conversation history to maintain context. Rely only on the provided documents and history.

[Instructions]
1. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ë¬¸ë§¥ì—ì„œ ì‹ ì¤‘í•˜ê²Œ í™•ì¸í•©ë‹ˆë‹¤.
2. ë‹µë³€ì— ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ì •ë³´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
3. ë¬¸ë§¥ì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì— ëŒ€í•´ ì¶”ì¸¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
4. ë¶ˆí•„ìš”í•œ ì •ë³´ë¥¼ í”¼í•˜ê³ , ëª…í™•í•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.
5. ë¬¸ë§¥ì—ì„œ ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ë‹¤ë©´ ë§ˆì§€ë§‰ì— "ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë”ìš± ëª…ì¾Œí•œ ë‹µë³€ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.     
""".strip()),
    ("system", "[Context]\n{context}"),
    ("system", "[History]\n{formatted_history}"),
    ("human", "{question}")
])

def generate_self(state: SelfRagOverallState) -> dict:
    print("--- ë‹µë³€ ìƒì„± (íˆìŠ¤í† ë¦¬ í¬í•¨) ---")
    
    # ìµœê·¼ ëŒ€í™” ì œí•œ
    recent_history = state["history"][10:] if len(state["history"]) > 5 else state["history"][:5]

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
    formatted_history = ""
    for user_msg, assistant_msg in recent_history:
        formatted_history += f"User: {user_msg}\nAssistant: {assistant_msg}\n\n"

    # 2) context ì§ë ¬í™”
    docs = state['filtered_documents'] or state['documents']
    context = "\n\n".join(d.page_content for d in docs) if docs else "ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ"

    # 3) í”„ë¡¬í”„íŠ¸ì— ê°’ ë„£ê³  LLM í˜¸ì¶œ
    chain = generate_template  | llm | StrOutputParser()
    out = chain.invoke({
        "formatted_history": formatted_history,
        "context": context,
        "question": state["question"],
    })
    answer: str = out

    # 4) ìƒíƒœ ì—…ë°ì´íŠ¸
    state["num_generations"] += 1
    state["generation"] = answer

    return {
        "generation": [answer],
        "num_generations": state["num_generations"],
    }

structured_llm_RoutingDecision = llm.with_structured_output(RoutingDecision)

question_router_system  = """
You are an AI assistant that routes user questions to the appropriate processing path.
Return one of the following labels:
- search_data
- llm_fallback
"""

question_router_prompt = ChatPromptTemplate.from_messages([
    ("system", question_router_system),
    ("human", "{question}")
])

question_router = question_router_prompt | structured_llm_RoutingDecision

# question route ë…¸ë“œ 
def route_question_adaptive(state: SelfRagOverallState) -> dict:
    print("--- ì§ˆë¬¸ íŒë‹¨ (ì¼ë°˜ or ê¸ˆìœµ) ---")
    print(f"ì§ˆë¬¸: {state['question']}")
    decision = question_router.invoke({"question": state['question']})
    print("routing_decision:", decision.route)
    return {"routing_decision": decision.route}

# question route ë¶„ê¸° í•¨ìˆ˜ 
def route_question_adaptive_self(state: SelfRagOverallState) -> str:
    """
    ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ…: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ 'ê¸ˆìœµì§ˆë¬¸'ì¸ì§€ 'ì¼ë°˜ì§ˆë¬¸'ì¸ì§€ íŒë‹¨
    """
    try:
        if state['routing_decision'] == "llm_fallback":
            print("--- ì¼ë°˜ì§ˆë¬¸ìœ¼ë¡œ ë¼ìš°íŒ… ---")
            return "llm_fallback"
        else:
            print("--- ê¸ˆìœµì§ˆë¬¸ìœ¼ë¡œ ë¼ìš°íŒ… ---")
            return "search_data"
    except Exception as e:
        print(f"--- ì§ˆë¬¸ ë¶„ì„ ì¤‘ Exception ë°œìƒ: {e} ---")
        return "llm_fallback"


fallback_prompt = ChatPromptTemplate.from_messages([
    ("system", """
    You are an AI assistant helping with various topics. 
    Respond in Korean.
    - Provide accurate and helpful information.
    - Keep answers concise yet informative.
    - Inform users they can ask for clarification if needed.
    - Let users know they can ask follow-up questions if needed.
    - End every answer with the sentence: "ì €ëŠ” ê¸ˆìœµìƒí’ˆ ì§ˆë¬¸ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê¸ˆìœµìƒí’ˆê´€ë ¨ ì§ˆë¬¸ì„ ì£¼ì„¸ìš”."
    """.strip()),
    ("system", "[History]\n{formatted_history}"),
    ("human", "{question}")
])

def llm_fallback_adaptive(state: SelfRagOverallState) -> dict:
    """Generates a direct response using the LLM when the question is unrelated to financial products."""
    print("--- ì¼ë°˜ ì§ˆë¬¸ Fallback (íˆìŠ¤í† ë¦¬ ë°˜ì˜) ---")
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
    formatted_history = ""
    for user_msg, assistant_msg in state["history"]:
        formatted_history += f"User: {user_msg}\nAssistant: {assistant_msg}\n\n"


    fallback_chain = fallback_prompt | llm | StrOutputParser()
    out = fallback_chain.invoke({
        "formatted_history": formatted_history,
        "question": state["question"],
    })
    answer: str = out

    state["history"].append((state["question"], answer))
    return {"generation": [answer]}

#############################
# 7. [ì„œë¸Œ ê·¸ë˜í”„ í†µí•©] - ë³‘ë ¬ ê²€ìƒ‰ ì„œë¸Œ ê·¸ë˜í”„ êµ¬í˜„
#############################

# --- ìƒíƒœ ì •ì˜ (ê²€ìƒ‰ ì„œë¸Œ ê·¸ë˜í”„ ì „ìš©) ---
class SearchState(TypedDict):
    question: str
    documents: Annotated[List[Document], add]  # íŒ¬ì•„ì›ƒëœ ê° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ëˆ„ì í•  ê²ƒ
    filtered_documents: List[Document]         # ê´€ë ¨ì„± í‰ê°€ë¥¼ í†µê³¼í•œ ë¬¸ì„œë“¤

# ToolSearchState: SearchStateì— ì¶”ê°€ ì •ë³´(datasources) í¬í•¨
class ToolSearchState(SearchState):
    datasources: List[str]  # ì°¸ì¡°í•  ë°ì´í„° ì†ŒìŠ¤ ëª©ë¡

# --- ì„œë¸Œê·¸ë˜í”„ ë…¸ë“œ í•¨ìˆ˜ ---
def search_fixed_deposit_node(state: SearchState):
    """
    ì •ê¸°ì˜ˆê¸ˆ ìƒí’ˆ ê²€ìƒ‰ (ì„œë¸Œ ê·¸ë˜í”„)
    """
    docs = search_fixed_deposit.invoke(state["question"])
    return {"documents": docs}

def search_demand_deposit_node(state: SearchState):
    """
    ì…ì¶œê¸ˆììœ ì˜ˆê¸ˆ ìƒí’ˆ ê²€ìƒ‰ (ì„œë¸Œ ê·¸ë˜í”„)
    """
    docs = search_demand_deposit.invoke(state["question"])
    return {"documents": docs}

def search_savings_node(state: SearchState):
    """
    ì ê¸ˆ ìƒí’ˆ ê²€ìƒ‰ (ì„œë¸Œ ê·¸ë˜í”„)
    """
    docs = search_savings.invoke(state["question"])
    return {"documents":docs}

def search_loan_node(state: SearchState):
    """
    ëŒ€ì¶œ ìƒí’ˆ ê²€ìƒ‰ (ì„œë¸Œ ê·¸ë˜í”„)
    """
    docs = search_loan.invoke(state["question"])
    return {"documents":docs}

def search_web_search_subgraph(state: SearchState):
    """
    ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ê¸ˆìœµ ì •ë³´ ê²€ìƒ‰ (ì„œë¸Œ ê·¸ë˜í”„)
    """
    question = state["question"]
    print('--- ì›¹ ê²€ìƒ‰ ì‹¤í–‰ ---')

    docs = web_search.invoke({"query": question})  # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë„˜ê¹€

    if len(docs) > 0:
        return {"documents": docs}
    else:
        return {"documents": [Document(page_content="ê´€ë ¨ ì›¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]}
    

def filter_documents_subgraph(state: SearchState):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì— ëŒ€í•´ ê´€ë ¨ì„± í‰ê°€ í›„ í•„í„°ë§
    """
    print("--- ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ (ì„œë¸Œ ê·¸ë˜í”„) ---")
    question = state["question"]
    documents = state["documents"]
    filtered_docs = []

    for d in documents:
        score = retrieval_grader_binary.invoke({
            "question": question,
            "document": d.page_content
        })
        if score.binary_score == "yes":
            print("--- ë¬¸ì„œ ê´€ë ¨ì„±: ìˆìŒ ---")
            filtered_docs.append(d)
        else:
            print("--- ë¬¸ì„œ ê´€ë ¨ì„±: ì—†ìŒ ---")

    # ì§ˆë¬¸ì—ì„œ ìš”êµ¬ë˜ëŠ” ì€í–‰ëª…(ì—”í‹°í‹°) ì¶”ì¶œ ë° í‘œì¤€í™”
    requested_banks = extract_and_normalize_banks(question)
    # ê´€ë ¨ì„± í†µê³¼ëœ ë¬¸ì„œì— ë“±ì¥í•œ ì€í–‰ëª… ì§‘í•© ì¶”ì¶œ
    found_banks = get_banks_in_docs(filtered_docs)

    # ===================== ì‹±ê¸€/ì—†ìŒ ì—”í‹°í‹° ë¶„ê¸° =====================
    if len(requested_banks) <= 1:
        # - ì§ˆë¬¸ì—ì„œ ì€í–‰ëª…ì´ 1ê°œ ì´í•˜ë¡œ ì¶”ì¶œëœ ê²½ìš°
        # - (1) ê´€ë ¨ì„± í‰ê°€ë§Œ í•œ ê²°ê³¼(filtered_docs) ë°˜í™˜
        # - (2) ëˆ„ë½ ì€í–‰ ë³´ì™„, ì¬ê²€ìƒ‰ ë“± ì¶”ê°€ ë¡œì§ "ìƒëµ"
        return {"filtered_documents": filtered_docs}
    
    # ===================== ë©€í‹° ì—”í‹°í‹°(2ê°œ ì´ìƒ) ë¶„ê¸° =====================
    missing_banks = [b for b in requested_banks if b not in found_banks]
    PRODUCT_CATEGORIES = ["ì •ê¸°ì˜ˆê¸ˆ", "ì…ì¶œê¸ˆììœ ì˜ˆê¸ˆ", "ì ê¸ˆ", "ëŒ€ì¶œ"]

    # (stateì— ëˆ„ë½ ì€í–‰ ë³´ì™„ íšŸìˆ˜ ê´€ë¦¬ìš© ë³€ìˆ˜ ì¶”ê°€)
    if "missing_bank_retry" not in state:
        state["missing_bank_retry"] = 0

    # (ì´ë¯¸ í™•ë³´í•œ ì€í–‰+ì¹´í…Œê³ ë¦¬ ìŒ ê´€ë¦¬)
    covered_pairs = set((doc.metadata.get("bank"), doc.metadata.get("type")) for doc in filtered_docs)

    # ========== (1) ëˆ„ë½ ì€í–‰ ë³´ì™„ ì¬ê²€ìƒ‰ ë¡œì§ (ì¼ë‹¨ ìµœëŒ€ íšŸìˆ˜ 1ê¹Œì§€ ì„¤ì •) ==========
    if missing_banks and state["missing_bank_retry"] < 1:
        # ëˆ„ë½ ì€í–‰ë§ˆë‹¤, ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¶”ê°€ ê²€ìƒ‰
        for bank in missing_banks:
            for category in PRODUCT_CATEGORIES:
                if (bank, category) in covered_pairs:
                    continue  # ì´ë¯¸ í™•ë³´ëœ ê²½ìš° ìƒëµ
                more_docs = hybrid_core_search(question, category=category, bank=bank, top_k=2)
                for d in more_docs:
                    if (d.metadata.get("bank"), d.metadata.get("type")) in covered_pairs:
                        continue
                    # ê´€ë ¨ì„± í‰ê°€(batchë¡œ í•  ìˆ˜ë„ ìˆìŒ)
                    score = retrieval_grader_binary.invoke({
                        "question": question,
                        "document": d.page_content
                    })
                    if score.binary_score == "yes":
                        filtered_docs.append(d)
                        covered_pairs.add((bank, category))
        state["missing_bank_retry"] += 1
        # í•œ ë²ˆ ë” ì»¤ë²„ë¦¬ì§€ ì²´í¬ í•˜ë„ë¡(ê·¸ë˜í”„ ë°˜ë³µ ë“±) ìƒíƒœ ë°˜í™˜
        return {"filtered_documents": filtered_docs, "missing_bank_retry": state["missing_bank_retry"]}

    # ========== (2) ë³´ì™„ 1íšŒ ì‹œë„ í›„ì—ë„ ëˆ„ë½ ì€í–‰ ë‚¨ì„ ê²½ìš° ==========
    if missing_banks and state["missing_bank_retry"] >= 1:
        # ë” ì´ìƒ ë³´ì™„ ì•ˆ í•˜ê³ , ì§€ê¸ˆê¹Œì§€ í™•ë³´í•œ ë¬¸ì„œë“¤ ì¤‘ ìƒìœ„ 3ê°œë§Œ ë‚¨ê¹€ (ì˜ˆì‹œ)
        filtered_docs = filtered_docs[:3]
        return {"filtered_documents": filtered_docs, "missing_bank_retry": state["missing_bank_retry"]}

    # (ëª¨ë“  ì€í–‰ì´ ì»¤ë²„ëê±°ë‚˜, ë³´ì™„ì´ ë¶ˆí•„ìš”í•œ ê²½ìš°)
    return {"filtered_documents": filtered_docs, "missing_bank_retry": state["missing_bank_retry"]}


# --- ì§ˆë¬¸ ë¼ìš°íŒ… (ì„œë¸Œ ê·¸ë˜í”„ ì „ìš©) ---
class SubgraphToolSelector(BaseModel):
    """Selects the most appropriate tool for the user's question."""
    tool: Literal["search_fixed_deposit", "search_demand_deposit", "search_loan","search_savings", "web_search"] = Field(
        description="Select one of the tools: search_fixed_deposit, search_demand_deposit, search_loan, search_savings or web_search based on the user's question."
    )

class SubgraphToolSelectors(BaseModel):
    """Selects all tools relevant to the user's question."""
    tools: List[SubgraphToolSelector] = Field(
        description="Select one or more tools: search_fixed_deposit, search_demand_deposit, search_loan, search_savings or web_search based on the user's question."
    )

structured_llm_SubgraphToolSelectors = llm.with_structured_output(SubgraphToolSelectors)

subgraph_system  = dedent("""\
You are an AI assistant specializing in routing user questions to the appropriate tools.
Use the following guidelines:
- For fixed deposit product queries, use the search_fixed_deposit tool.
- For demand deposit product queries, use the search_demand_deposit tool.
- For loan product queries, use the search_loan tool.
- For savings product queries, use the search_savings tool.
- For general financial or real-time information queries, or when the user explicitly mentions 'web search',
  use the web_search tool.
  Always choose the appropriate tools based on the user's question.
""")
subgraph_route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", subgraph_system),
        ("human", "{question}")
    ]
)
question_tool_router = subgraph_route_prompt  | structured_llm_SubgraphToolSelectors

def analyze_question_tool_search(state: ToolSearchState):
    """
    ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ…: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ì°¸ì¡°í•  ë°ì´í„° ì†ŒìŠ¤ ê²°ì •
    """
    print("--- ì§ˆë¬¸ ë¼ìš°íŒ… ---")
    question = state["question"]
    result = question_tool_router.invoke({"question": question})
    datasources = [tool.tool for tool in result.tools]
    return {"datasources": datasources}

def route_datasources_tool_search(state: ToolSearchState) -> Sequence[str]:
    """
    ë¼ìš°íŒ… ê²°ê³¼ì— ë”°ë¼ ì‹¤í–‰í•  ê²€ìƒ‰ ë…¸ë“œë¥¼ ê²°ì • (ë³‘ë ¬ë¡œ íŒ¬ì•„ì›ƒ)
    """
    datasources = set(state['datasources'])
    print("--- ì„ íƒëœ ê²€ìƒ‰ ë„êµ¬ ---")
    print(datasources)
    # ëª…í™•íˆ í•˜ë‚˜ë§Œ ì„ íƒëœ ê²½ìš°
    if datasources == {'search_fixed_deposit'}:
        return ['search_fixed_deposit']
    elif datasources == {'search_demand_deposit'}:
        return ['search_demand_deposit']
    elif datasources == {"search_loan"}:
        return ['search_loan']
    elif datasources == {"search_savings"}:
        return ['search_savings']
    elif datasources == {'web_search'}:
        return ['web_search']

    # ë„êµ¬ê°€ ì „ë¶€ ì‹¤í–‰ë˜ê±°ë‚˜ ì• ë§¤ëª¨í˜¸í•  ë•ŒëŠ” ë„êµ¬ ì „ë¶€ ì‹¤í–‰
    return ['search_fixed_deposit', 'search_demand_deposit', 'search_loan', 'search_savings', 'web_search']



# --- ì„œë¸Œ ê·¸ë˜í”„ ë¹Œë” êµ¬ì„± ---
search_builder = StateGraph(ToolSearchState)

# ë…¸ë“œ ì¶”ê°€
search_builder.add_node("analyze_question", analyze_question_tool_search)
search_builder.add_node("search_fixed_deposit", search_fixed_deposit_node)   
search_builder.add_node("search_demand_deposit", search_demand_deposit_node) 
search_builder.add_node("search_loan",search_loan_node)
search_builder.add_node("search_savings",search_savings_node)
search_builder.add_node("web_search", search_web_search_subgraph)
search_builder.add_node("filter_documents", filter_documents_subgraph)

# ì—£ì§€ êµ¬ì„±
search_builder.add_edge(START, "analyze_question")
search_builder.add_conditional_edges(
    "analyze_question",
    route_datasources_tool_search,
    {
        "search_fixed_deposit": "search_fixed_deposit",
        "search_demand_deposit": "search_demand_deposit",
        "search_loan": "search_loan",
        "search_savings": "search_savings",
        "web_search": "web_search"
    }
)
# ë‘ ê²€ìƒ‰ ë…¸ë“œ ëª¨ë‘ ì‹¤í–‰í•œ í›„ ê°ê°ì˜ ê²°ê³¼ëŠ” filter_documentsë¡œ íŒ¬ì¸(fan-in) ì²˜ë¦¬
search_builder.add_edge("search_fixed_deposit", "filter_documents")
search_builder.add_edge("search_demand_deposit", "filter_documents")
search_builder.add_edge("search_loan","filter_documents")
search_builder.add_edge("search_savings","filter_documents")
search_builder.add_edge("web_search", "filter_documents")
search_builder.add_edge("filter_documents", END)

# ì„œë¸Œ ê·¸ë˜í”„ ì»´íŒŒì¼
tool_search_graph = search_builder.compile()

#############################
# 8. [ì „ì²´ ê·¸ë˜í”„ì™€ ê²°í•©] - Self-RAG Overall Graph
#############################
print('\n8. [ì „ì²´ ê·¸ë˜í”„ì™€ ê²°í•©] - Self-RAG Overall Graph\n')

# ì „ì²´ ê·¸ë˜í”„ ë¹Œë” (rag_builder) êµ¬ì„±
rag_builder = StateGraph(SelfRagOverallState)

# ë…¸ë“œ ì¶”ê°€: ê²€ìƒ‰ ì„œë¸Œ ê·¸ë˜í”„, ìƒì„±, ì§ˆë¬¸ ì¬ì‘ì„± ë“±
rag_builder.add_node("contextualize_query", contextualize_query)
rag_builder.add_node("route_question", route_question_adaptive)
rag_builder.add_node("llm_fallback", llm_fallback_adaptive)
rag_builder.add_node("search_data", tool_search_graph)         # ì„œë¸Œ ê·¸ë˜í”„ë¡œ ë³‘ë ¬ ê²€ìƒ‰ ë° í•„í„°ë§ ìˆ˜í–‰
rag_builder.add_node("generate", generate_self)                # ë‹µë³€ ìƒì„± ë…¸ë“œ
rag_builder.add_node("transform_query", transform_query_self)  # ì§ˆë¬¸ ê°œì„  ë…¸ë“œ

# ì „ì²´ ê·¸ë˜í”„ ì—£ì§€ êµ¬ì„±
rag_builder.add_edge(START, "contextualize_query")
rag_builder.add_edge("contextualize_query", "route_question")
rag_builder.add_conditional_edges(
    "route_question",
    route_question_adaptive_self, 
    {
        "llm_fallback": "llm_fallback",
        "search_data": "search_data"
    }
)
rag_builder.add_edge("llm_fallback", END)
rag_builder.add_conditional_edges(
    "search_data",
    decide_to_generate_self, 
    {
        "transform_query": "transform_query",
        "generate": "generate",
    }
)
rag_builder.add_edge("transform_query", "search_data")
rag_builder.add_conditional_edges(
    "generate",
    grade_generation_self,
    {
        "not supported": "generate",      # í™˜ê° ë°œìƒ ì‹œ ì¬ìƒì„±
        "not useful": "transform_query",  # ê´€ë ¨ì„± ë¶€ì¡± ì‹œ ì§ˆë¬¸ ì¬ì‘ì„± í›„ ì¬ê²€ìƒ‰
        "useful": END,
        "end": END,
    }
)

# MemorySaver ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ëŒ€í™” ìƒíƒœë¥¼ ì €ì¥í•  in-memory í‚¤-ê°’ ì €ì¥ì†Œ)
memory = MemorySaver()
adaptive_self_rag_memory = rag_builder.compile(checkpointer=memory)

# ê·¸ë˜í”„ íŒŒì¼ ì €ì¥í•˜ê¸°
with open("adaptive_self_rag_memory.mmd", "w") as f:
    f.write(adaptive_self_rag_memory.get_graph(xray=True).draw_mermaid()) # ì €ì¥ëœ mmd íŒŒì¼ì—ì„œ ì½”ë“œ ë³µì‚¬ í›„ https://mermaid.live ì— ë¶™ì—¬ë„£ê¸°.


#############################
# 9. Gradio Chatbot êµ¬ì„± ë° ì‹¤í–‰
#############################

# pdf_link ì‚½ì… ë³´ì¡°í•¨ìˆ˜
def postprocess_answer(answer: str, docs: List[Document]) -> str:
    for doc in docs:
        pdf = doc.metadata.get("pdf_link")
        if pdf:
            if "ìƒí’ˆì„¤ëª…ì„œ" not in answer:
                answer += f"\n\n [ìƒí’ˆì„¤ëª…ì„œ PDF ë³´ê¸°]({pdf})"
            break
    return answer


# ì±—ë´‡ í´ë˜ìŠ¤
class ChatBot:
    def __init__(self):
        self.thread_id = str(uuid.uuid4())

    def chat(self, message: str, history: List[Tuple[str, str]]) -> str:
        """
        ì…ë ¥ ë©”ì‹œì§€ì™€ ëŒ€í™” ì´ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ Adaptive Self-RAG ì²´ì¸ì„ í˜¸ì¶œí•˜ê³ ,
        ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        config = {"configurable": {"thread_id": self.thread_id}}
        state = initialize_state()
        state["question"] = message
        
        # historyê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if history:
            state["history"] = history
        
        result = adaptive_self_rag_memory.invoke(state, config=config)
        gen_list = result.get("generation", [])
        docs = result.get("filtered_documents", [])
        if not gen_list:
            bot_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            raw_answer = gen_list[-1]
            bot_response = postprocess_answer(raw_answer, docs)

        # ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸
        print(f"--- History í™•ì¸ ---\n{state['history']}")
        return bot_response


# ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
chatbot = ChatBot() 

# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
demo = gr.ChatInterface(
    fn=chatbot.chat,
    title="Adaptive Self-RAG ê¸°ë°˜ RAG ì±—ë´‡ ì‹œìŠ¤í…œ",
    description="ì˜ˆê¸ˆ, ì ê¸ˆ, ì‹ ìš©ëŒ€ì¶œ ìƒí’ˆ ë° ê¸°íƒ€ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.",
    examples=[
        "ì •ê¸°ì˜ˆê¸ˆ ìƒí’ˆ ì¤‘ ê¸ˆë¦¬ê°€ ê°€ì¥ ë†’ì€ ê²ƒì€?",
        "ì •ê¸°ì˜ˆê¸ˆê³¼ ì…ì¶œê¸ˆììœ ì˜ˆê¸ˆì€ ì–´ë–¤ ì°¨ì´ì ì´ ìˆë‚˜ìš”?",
        "ì€í–‰ì˜ ì˜ˆê¸ˆ ìƒí’ˆì„ ì¶”ì²œí•´ ì£¼ì„¸ìš”."
    ],
    theme=gr.themes.Soft()
)

# Gradio ì•± ì‹¤í–‰: ì´ íŒŒì¼ì„ ë©”ì¸ìœ¼ë¡œ ì‹¤í–‰í•  ë•Œë§Œ ë„ì›ë‹ˆë‹¤.
if __name__ == "__main__":
    demo.launch(share=True)